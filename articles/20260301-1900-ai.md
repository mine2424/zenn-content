---
title: "LLMのコンテキスト管理を制する者がAIエージェントを制する"
emoji: "🧠"
type: "tech"
topics: ["llm", "aiagent", "prompt", "openai", "ai"]
published: false
---

## はじめに

LLMを使ったアプリケーション開発を始めると、早い段階で必ず直面する問題がある。それは「コンテキストウィンドウの限界」だ。

GPT-4やClaudeのような最新モデルでも、一度に扱えるトークン数には上限がある。テキストを大量に流し込めばモデルが賢く答えてくれる、という単純な話ではない。むしろ、**何を渡して、何を渡さないか**を設計することが、LLMアプリケーションの品質を左右する最も重要な判断になる。

この記事では、コンテキスト管理の基本的な考え方と、AIエージェントを設計するうえで見落とされがちな視点を掘り下げていく。

## コンテキストとは何か

まず「コンテキスト」という言葉を整理しよう。LLMの文脈では、コンテキストとは**モデルが回答を生成する際に参照できる情報の総体**を指す。

一般的なチャット型アプリでは：

- システムプロンプト（ロール定義・制約・ルール）
- 会話履歴（ユーザーとアシスタントのやり取り）
- 必要に応じて取得したドキュメントや検索結果

これらをまとめてモデルに渡す。問題は、会話が長くなればなるほど、また取得情報が増えれば増えるほど、コンテキストが肥大化していくことだ。

## なぜコンテキストの過剰投入が問題なのか

「長ければ長いほど情報が多くて良いのでは？」と思うかもしれないが、そうではない。理由は主に3つある。

**1. コスト**  
LLMのAPIはほぼすべてトークン単位の課金だ。入力トークンが増えれば増えるほど、コストは直線的に上がる。エージェントがツールを呼び出すたびに毎回すべての会話履歴を含めると、コストは急速に膨らむ。

**2. モデルの注意力の分散（Lost in the Middle問題）**  
研究によって明らかになっているが、LLMはコンテキストの先頭と末尾の情報は比較的よく使うが、**中間部分の情報を見落としやすい**。コンテキストを長くするほど、重要な情報が中間に埋もれて無視されるリスクが高まる。

**3. 応答速度の低下**  
入力トークンが増えるほど推論時間も伸びる。リアルタイム性が求められるアプリでは、コンテキストの肥大化は直接ユーザー体験の劣化につながる。

## コンテキスト管理の3つのアプローチ

では、どうやってコンテキストをコントロールするか。主なアプローチを3つ挙げる。

### 1. スライディングウィンドウ

最も単純な方法は、会話履歴を「直近N件」に制限することだ。古い会話は切り捨て、常に最新の会話だけをコンテキストに含める。

実装は簡単だが、古い会話の内容が失われるため、会話の一貫性が損なわれる場合がある。短期的なタスク完結型のチャットボットには向いているが、長い文脈を維持する必要があるエージェントには不向きだ。

### 2. サマライゼーション（要約）

一定量の会話が溜まったら、LLM自身に古い会話を要約させ、その要約を履歴の代わりに保持する手法だ。

```python
def summarize_history(messages: list[dict]) -> str:
    summary_prompt = [
        {"role": "user", "content": f"以下の会話を簡潔に要約してください:\n{format_messages(messages)}"}
    ]
    response = llm.complete(summary_prompt)
    return response.content
```

情報の圧縮効率は高く、長期の文脈保持にも対応できる。ただし要約の品質がモデル依存であり、細かいニュアンスが失われるリスクがある。

### 3. 構造化メモリ（外部記憶）

会話の内容から重要な情報をエンティティとして抽出し、外部データベースに保存する。必要なタイミングで検索して取得する方式だ。RAG（Retrieval-Augmented Generation）の考え方を会話履歴に応用したものと言える。

これは最も柔軟だが、実装コストも高い。エンティティ抽出のロジックや検索品質がそのままシステムの品質に直結する。

## AIエージェントにおけるコンテキスト設計

単純なチャットボットと違い、AIエージェントはツール呼び出しや複数ステップの推論を繰り返す。このため、コンテキスト管理はさらに複雑になる。

考えるべき重要な観点は「**何がエージェントの判断に必要か**」だ。

エージェントは次のアクションを決定するために、以下の情報を必要とする：

- **ゴール**：最終的に何を達成すべきか
- **現在の状態**：どこまで進んでいるか
- **利用可能なツール**：何ができるか
- **直近のアクション結果**：直前のステップで何が起きたか

逆に言えば、これ以外の情報はノイズになりうる。特に「直近のアクション結果」だけでなく「過去の全アクション結果」を毎回全部渡すのは非効率だ。

実際のエージェント設計では、**タスクの進行状態を構造化されたオブジェクトで管理**し、コンテキストには「現在の状態スナップショット」だけを渡すアプローチが有効だ。

## プロンプト設計との関係

コンテキスト管理はプロンプト設計とも密接に関係している。プロンプトの書き方によって、同じ情報量でもモデルの理解度が大きく変わる。

特に意識したいのは**情報の配置**だ。重要な制約やゴールはコンテキストの先頭（システムプロンプト）に明記し、変動する情報（取得結果や状態）はユーザーターンに置く。この配置原則だけで、モデルが重要情報を見落とすリスクをかなり下げられる。

また、プロンプトには「返答形式の指定」も含めると良い。JSON形式での出力を求める場合、単に「JSONで返して」と書くより、実際のJSONスキーマを示す方がはるかに精度が上がる。

## まとめ：設計の哲学として

コンテキスト管理を「技術的な制約への対応」として捉えると、どうしても後手に回る。むしろ「**モデルに何を見せるかを設計する行為**」として正面から向き合うことが重要だ。

LLMはブラックボックスだが、入力の設計は完全にこちらの手の内にある。コンテキストの中身を丁寧に設計することは、プロンプトエンジニアリングの核心でもあり、AIエージェント開発のアーキテクチャ設計の核心でもある。

良いLLMアプリケーションとは、「賢いモデルを使っているもの」ではなく「モデルが賢く見えるよう入力を設計できているもの」だ。この視点の転換から、コンテキスト管理への真剣な向き合い方が始まる。
